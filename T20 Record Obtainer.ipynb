{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ca30e66",
   "metadata": {},
   "source": [
    "This notebook is used to obtain all required data to be fed into the model. The data will be parsed from https://www.espncricinfo.com/, which is a cricket sports website, and a database. \n",
    "\n",
    "The outputs of this script are:\n",
    "\n",
    "1. match_url.csv (All T20I matches played prior to the World Cup including stats from the matches)\n",
    "2. match_data.csv (All players that were involved in matches recorded in 1.)\n",
    "3. player_data.csv (Statistics of all players that were involved in all recorded matches)\n",
    "4. player_match_data.csv (Same as 2. but includes each players country required for data preprocessing)\n",
    "5. player_squad_data.csv (Based on the selected squads for the tournement all statistics for selected players) \n",
    "\n",
    "Note: It is not recommened to run this script as the parsing takes siginicant time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22400e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all required libraries\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen as uReq\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d87ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TeamData Function takes as an input of each team and runs the ObtainData function\n",
    "def TeamData (team):\n",
    "    for i in range(1,10):\n",
    "        ObtainData(str(team),str(i),f)\n",
    "\n",
    "# The ObtainData function runs through all teams match history and records all games played by the team. \n",
    "# Each team page follows the same url structure and the input of the function is changed to allow different teams to be recorded\n",
    "# The function looks for a specific string in the page HTML and based on that stores certain values for each team\n",
    "def ObtainData(team,page,f):\n",
    "    url = 'https://stats.espncricinfo.com/ci/engine/stats/index.html?class=3;page='+page+';team='+team+';template=results;type=team;view=results'\n",
    "    uClient = uReq(url)\n",
    "    raw_html = uClient.read()\n",
    "    uClient.close()   \n",
    "    page_soup = soup(raw_html, \"html.parser\")\n",
    "    data = page_soup.findAll(\"a\", string='Match scorecard')\n",
    "    table = page_soup.findAll(\"table\", {\"class\" : \"engineTable\"})[2].tbody\n",
    "    \n",
    "    store = table.find_all(\"tr\")\n",
    "    data = page_soup.findAll(\"a\", string='Match scorecard')\n",
    "    page_check = page_soup.findAll(\"tr\", {'class', 'data1'})[0].findAll('td', {'class', 'left'})[0].text\n",
    "    \n",
    "    for i in range(len(store)):\n",
    "        if (page_check == 'No records available to match this query'):\n",
    "            break\n",
    "        else:\n",
    "            results_check = page_soup.findAll(\"tr\", {'class', 'data1'})[i].findAll('td', {'class', 'left'})[1].text\n",
    "            if results_check == 'won' or results_check == 'lost':\n",
    "                f.write(\"\\n\")\n",
    "                match_url = 'https://www.espncricinfo.com' + data[i]['href'] \n",
    "                insidedata = store[i].find_all(\"td\")\n",
    "                f.write(match_url + \",\")\n",
    "                for k in range(len(insidedata)):\n",
    "                    if (insidedata[k].text == 'No records available to match this query'):\n",
    "                        break\n",
    "                    else:\n",
    "                        f.write(insidedata[k].text + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command runs the above command for all teams listed in the dictionary and stores it into a file. \n",
    "teams = {'40': 'Afghanistan', '2' : 'Australia', '25' : 'Bangladesh', '1':'England','6':'India','29':'Ireland',\n",
    "         '28':'Namibia','15':'Netherlands','5':'NewZealand','37':'Oman','7':'Pakistan','20':'P.N.G.',\n",
    "         '30':'Scotland','3':'SouthAfrica','8':'SriLanka','4':'WestIndies'}\n",
    "\n",
    "filename = \"match_url.csv\"\n",
    "\n",
    "f = open(filename, 'w')\n",
    "headers = \"match_url,Team, Result, Margin, BR, Toss, Bat, ,Opposition, Ground, Date\\n\"\n",
    "f.write(headers)\n",
    "\n",
    "for key in teams.keys():\n",
    "    TeamData(key)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc09f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to record all urls of the players that have played in any recorded matches.\n",
    "def player_url_obtainer(match):\n",
    "  \n",
    "    for i in range(len(match)):\n",
    "        url = match['match_url'][i]\n",
    "        uClient = uReq(url)\n",
    "        raw_html = uClient.read()\n",
    "        uClient.close()   \n",
    "        page_soup = soup(raw_html, \"html.parser\")\n",
    "\n",
    "        name_finder=page_soup.findAll(\"a\", {\"title\" :re.compile(r\"View full profile of \")})\n",
    "        \n",
    "        for i in range(len(name_finder)):\n",
    "            player_url.append('https://www.espncricinfo.com' + name_finder[i]['href'])\n",
    "\n",
    "\n",
    "    for i in range(len(player_url)):\n",
    "        f.write(\"\\n\")\n",
    "        f.write(player_url[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ce8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the match_url generated earlier and create new data with only match_url tat can be used in the player_url_obtainer function.\n",
    "match_url = pd.read_csv('match_url.csv')\n",
    "match_url = match_url.reset_index()\n",
    "match_url.columns = ['match_url','Team','Result','Margin','BR','Toss','Bat', 'Unnamed1','Opposition', 'Ground','Date','Unnamed2','Unnamed3']\n",
    "match_url = match_url.drop_duplicates(subset=['match_url'])\n",
    "match_url = match_url.reset_index()\n",
    "match = match_url.drop(['index','Unnamed1', 'Unnamed2','Unnamed3'],axis = 1)\n",
    "filename = \"player_url.csv\"\n",
    "f = open(filename, 'w')\n",
    "headers = \"player_url\"\n",
    "f.write(headers)\n",
    "player_url = []\n",
    "player_url_obtainer(match)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1369b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the player url this function takes all data from each players url\n",
    "# Care had to be taken in order of structure of data, that is the reason for the four if loops and the mention of T20I\n",
    "def player_data_obtainer(playerdf,f):\n",
    "    \n",
    "    for i in range(len(playerdf)):\n",
    "        url = playerdf['player_url'][i]\n",
    "        uClient = uReq(url)\n",
    "        raw_html = uClient.read()\n",
    "        uClient.close()   \n",
    "        page_soup = soup(raw_html, \"html.parser\")\n",
    "\n",
    "        f.write(\"\\n,\")\n",
    "        f.write(page_soup.findAll(\"h5\", {\"class\" : \"player-card-description gray-900\"})[0].text + \",\")\n",
    "        f.write(page_soup.findAll(\"span\", {\"class\" : \"player-card__country-name\"})[0].text + \",\")\n",
    "\n",
    "        tablefinder = page_soup.findAll('div' ,{'class': 'card overflow-hidden mb-3'})\n",
    "\n",
    "        if tablefinder[1].findAll('h5')[0].text == 'Batting & Fielding':\n",
    "            statfinder = page_soup.findAll('span')\n",
    "            for i in range(len(statfinder)):\n",
    "                if statfinder[i].text == 'T20I':\n",
    "                    for j in range(1,15):\n",
    "                        f.write(statfinder[i+j].text + \",\")\n",
    "                    break\n",
    "\n",
    "        if tablefinder[1].findAll('h5')[1].text == 'Batting & Fielding':\n",
    "            count = 0\n",
    "            statfinder = page_soup.findAll('span')\n",
    "            for i in range(len(statfinder)):\n",
    "                if statfinder[i].text == 'T20I':\n",
    "                    count += 1\n",
    "                    if count == 1:\n",
    "                        continue\n",
    "                    if count == 2:\n",
    "                        for j in range(1,15):\n",
    "                            f.write(statfinder[i+j].text + \",\")\n",
    "                        break\n",
    "\n",
    "        if tablefinder[1].findAll('h5')[0].text == 'Bowling':\n",
    "            count = 0\n",
    "            statfinder = page_soup.findAll('span')\n",
    "            for i in range(len(statfinder)):\n",
    "                if statfinder[i].text == 'T20I':\n",
    "                    for j in range(1,14):\n",
    "                        f.write(statfinder[i+j].text + \",\")\n",
    "                    break\n",
    "\n",
    "        if tablefinder[1].findAll('h5')[1].text == 'Bowling':\n",
    "            count = 0\n",
    "            statfinder = page_soup.findAll('span')\n",
    "            for i in range(len(statfinder)):\n",
    "                if statfinder[i].text == 'T20I':\n",
    "                    count += 1\n",
    "                    if count == 1:\n",
    "                        continue\n",
    "                    if count == 2:\n",
    "                        for j in range(1,14):\n",
    "                            f.write(statfinder[i+j].text + \",\")\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5801503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores the recorded data from the above function into a new csv with the required data\n",
    "player_url = pd.read_csv('player_url.csv')\n",
    "player_url = player_url.drop_duplicates()\n",
    "player_url = player_url.reset_index()\n",
    "\n",
    "filename = \"player_data.csv\"\n",
    "f = open(filename, 'w')\n",
    "headers = \",Player_Name,Country,Mat_Bat,Inns_Bat,NO,Runs,HS,Ave,BF,SR,100s,50s,4s,6s,Ct,St,Mat_Bowl,Inns_Bowl,Balls,Runs,Wkts,BBI,BBM,Ave,Econ,SR,4w,5w,10w\"\n",
    "f.write(headers)\n",
    "player_data_obtainer(player_url,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  From each match scorecard the players names are obtained this removes any additional strings in the name, such as (c)\n",
    "def GetMatchInformation(match, f):\n",
    "    for i in range(len(match)):\n",
    "\n",
    "        url = match['match_url'][i]\n",
    "        uClient = uReq(url)\n",
    "        raw_html = uClient.read()\n",
    "        uClient.close()   \n",
    "        page_soup = soup(raw_html, \"html.parser\")\n",
    "\n",
    "        name_finder=page_soup.findAll(\"a\", {\"title\" :re.compile(r\"View full profile of \")})\n",
    "        for i in range(len(name_finder)):\n",
    "            f.write(\"\\n,\")\n",
    "            f.write(url + \",\")\n",
    "            name = name_finder[i].text\n",
    "            name = re.sub('(c)', '', name)\n",
    "            name = re.sub('[^A-Za-z0-9 ]+', '', name)\n",
    "            f.write(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the inputted csv to input only match_url into the GetMatchInformation function. \n",
    "match_url = pd.read_csv('match_url.csv')\n",
    "match_url = match_url.reset_index()\n",
    "match_url.columns = ['match_url','Team','Result','Margin','BR','Toss','Bat', 'Unnamed1','Opposition', 'Ground','Date','Unnamed2','Unnamed3']\n",
    "match_url = match_url.drop_duplicates(subset=['match_url'])\n",
    "match_url = match_url.reset_index()\n",
    "match = match_url.drop(['index','Unnamed1', 'Unnamed2','Unnamed3'],axis = 1)\n",
    "\n",
    "filename = \"match_data.csv\"\n",
    "f = open(filename, 'w')\n",
    "headers = \",match_url,player_name\"\n",
    "f.write(headers)\n",
    "GetMatchInformation(match, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6ae9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the GetMatchInformation, this function below takes all player name and country from the player url page\n",
    "def GetPlayerInformation(match, f):\n",
    "    for i in range(len(match)):\n",
    "        \n",
    "        print(\"Count= \" + str(i))\n",
    "\n",
    "        url = match['match_url'][i]\n",
    "        uClient = uReq(url)\n",
    "        raw_html = uClient.read()\n",
    "        uClient.close()   \n",
    "        page_soup = soup(raw_html, \"html.parser\")\n",
    "\n",
    "        name_finder=page_soup.findAll(\"a\", {\"title\" :re.compile(r\"View full profile of \")})\n",
    "        for i in range(len(name_finder)):\n",
    "            player_url = ('https://www.espncricinfo.com' + name_finder[i]['href'])\n",
    "            uClient = uReq(player_url)\n",
    "            second_html = uClient.read()\n",
    "            uClient.close()   \n",
    "            new_soup = soup(second_html, \"html.parser\")\n",
    "            f.write(\"\\n,\")\n",
    "            f.write(url + \",\")\n",
    "            f.write(new_soup.findAll(\"h5\", {\"class\" : \"player-card-description gray-900\"})[0].text + \",\")\n",
    "            f.write(new_soup.findAll(\"span\", {\"class\" : \"player-card__country-name\"})[0].text + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d3db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the match_url.csv obtained earlier to remove 'v' from the team_2 column\n",
    "all_matches_with_data_df = pd.read_csv('match_url.csv')\n",
    "all_matches_with_data_df = all_matches_with_data_df.reset_index()\n",
    "headers = ['match_url', 'team_1', 'Result', 'winning_margin', 'BR', 'Toss', 'Bat', 'Unnamed0', 'team_2','Ground','Date','Unnamed1','Unnamed2']\n",
    "all_matches_with_data_df.columns = headers \n",
    "all_matches_with_data_df = all_matches_with_data_df.drop(['BR','Unnamed0','Ground','Unnamed1', 'Unnamed2'], axis = 1)\n",
    "all_matches_with_data_df['team_2'] = all_matches_with_data_df['team_2'].str.replace('v ','')\n",
    "all_matches_with_data_df = all_matches_with_data_df.drop_duplicates(subset=['match_url'])\n",
    "all_matches_with_data_df = all_matches_with_data_df.reset_index(drop=True)\n",
    "\n",
    "# Get the player_match_data information\n",
    "filename = \"player_match_data.csv\"\n",
    "f = open(filename, 'w')\n",
    "headers = \",match_url,player_name,player_country\"\n",
    "match = all_matches_with_data_df\n",
    "match = match.reset_index(drop=True)\n",
    "f.write(headers)\n",
    "GetPlayerInformation(match, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83ebe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command is used to find all the T20I world cup squads by prasing the ESPNCricinfo page\n",
    "filename = \"squads.csv\"\n",
    "f = open(filename, 'w')\n",
    "headers = \"squad_url,player_url\"\n",
    "f.write(headers)\n",
    "\n",
    "url = 'https://www.espncricinfo.com/series/icc-men-s-t20-world-cup-2021-22-1267897/squads'\n",
    "uClient = uReq(url)\n",
    "raw_html = uClient.read()\n",
    "uClient.close()   \n",
    "page_soup = soup(raw_html, \"html.parser\")\n",
    "squadurl = page_soup.findAll(\"a\", {\"class\" : \"black-link d-none d-md-inline-block pl-2\"})\n",
    "for i in range(len(squadurl)):\n",
    "    squad_url = 'https://www.espncricinfo.com' + squadurl[i]['href']\n",
    "    uClient = uReq(squad_url)\n",
    "    sqaud_raw_html = uClient.read()\n",
    "    uClient.close()   \n",
    "    page_soup_player = soup(sqaud_raw_html, \"html.parser\")\n",
    "    squad_player = page_soup_player.findAll(\"a\", {\"class\" : \"h3 benton-bold name black-link d-inline\"})\n",
    "    for j in range(len(squad_player)):\n",
    "        squad_player_url = 'https://www.espncricinfo.com' + squad_player[j]['href']\n",
    "        f.write(\"\\n\")\n",
    "        f.write(squad_url + \",\")\n",
    "        f.write(squad_player_url)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5daa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This obtains all the stats of the players in each squad using the same function as before\n",
    "player_squad_url = pd.read_csv('squads.csv')\n",
    "player_squad_url = player_squad_url.drop(['squad_url'],axis=1)\n",
    "filename = \"player_squad_data.csv\"\n",
    "f = open(filename, 'w')\n",
    "headers = \",Player_Name,Country,Mat_Bat,Inns_Bat,NO,Runs,HS,Ave,BF,SR,100s,50s,4s,6s,Ct,St,Mat_Bowl,Inns_Bowl,Balls,Runs,Wkts,BBI,BBM,Ave,Econ,SR,4w,5w,10w\"\n",
    "f.write(headers)\n",
    "player_data_obtainer(player_squad_url,f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
